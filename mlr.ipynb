{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A  | Simple and Multiple Linear Regression\n",
    "### Spring 2023, Week 5\n",
    "<hr>\n",
    "\n",
    "Objectives:\n",
    "-  Run and interpret a linear regression\n",
    "-  Gain intuition about MLR results\n",
    "-  Introduce concepts like overfitting and test data\n",
    "\n",
    "__Question:__ Assume you run an experiment observing the impact of 10 different microbial soil populations on the viability of Arabidopsis thaliana (plant). How many 2-sampled t-tests would you need to run to evaluate pairwise effect (or lack thereof) of the different soil populations?  \n",
    "\n",
    "$$ 10 \\times (10-1) / 2  = 45 $$\n",
    "\n",
    "__Question:__ Assume you run an experiment observing the impact of 10 different microbial soil populations on the viability and growth of Arabidopsis thaliana (plant model).\n",
    "\n",
    "Provided an alpha=0.05 as the threshold for your study, what is your bonferoni adjusted alpha for each 2-sample t-test? \n",
    "\n",
    "$$\\alpha/n_{tests} = 0.05/45 =  0.0011$$\n",
    "\n",
    "__False:__ The null distribution is the same for all null hypotheses.\n",
    "\n",
    "-  __T-Test__: Mean/Variance = _t_-distribution\n",
    "\n",
    "-  __ANOVA__: Variance/Variance = _F_-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/BIOL359A-FoundationsOfQBio-Spr23/week5_linearregression.git\n",
    "!mkdir ./data\n",
    "!cp week5_linearregression/data/* ./data\n",
    "!cp week5_linearregression/clean_data.py ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from scipy.stats import ttest_ind as ttest\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "TITLE_FONT = 20\n",
    "LABEL_FONT = 16\n",
    "TICK_FONT = 16\n",
    "FIG_SIZE = (12,12)\n",
    "COLORS= [\"#008080\",\"#CA562C\"]\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"whitegrid\",  {'axes.linewidth': 2, 'axes.edgecolor':'black'})\n",
    "sns.set(font_scale=1, rc={'figure.figsize':FIG_SIZE}) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to use the Wisconsin Diagnostic Breast Cancer dataset once again\n",
    "\n",
    "From the data source: Wisconsin Diagnostic Breast Cancer (WDBC)\n",
    "\n",
    "```\n",
    "\tFeatures are computed from a digitized image of a fine needle\n",
    "\taspirate (FNA) of a breast mass.  They describe\n",
    "\tcharacteristics of the cell nuclei present in the image.\n",
    "\tA few of the images can be found at\n",
    "\thttp://www.cs.wisc.edu/~street/images/\n",
    "\n",
    "\tSeparating plane described above was obtained using\n",
    "\tMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
    "\tConstruction Via Linear Programming.\" Proceedings of the 4th\n",
    "\tMidwest Artificial Intelligence and Cognitive Science Society,\n",
    "\tpp. 97-101, 1992], a classification method which uses linear\n",
    "\tprogramming to construct a decision tree.  Relevant features\n",
    "\twere selected using an exhaustive search in the space of 1-4\n",
    "\tfeatures and 1-3 separating planes.\n",
    "\n",
    "\tThe actual linear program used to obtain the separating plane\n",
    "\tin the 3-dimensional space is that described in:\n",
    "\t[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
    "\tProgramming Discrimination of Two Linearly Inseparable Sets\",\n",
    "\tOptimization Methods and Software 1, 1992, 23-34].\n",
    "    \n",
    "    Source:\n",
    "    W.N. Street, W.H. Wolberg and O.L. Mangasarian \n",
    "\tNuclear feature extraction for breast tumor diagnosis.\n",
    "\tIS&T/SPIE 1993 International Symposium on Electronic Imaging: Science\n",
    "\tand Technology, volume 1905, pages 861-870, San Jose, CA, 1993.\n",
    "```\n",
    "\n",
    "What do all the column names mean?\n",
    "\n",
    "- ID number\n",
    "- Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry \n",
    "- fractal dimension (\"coastline approximation\" - 1) - a measure of \"complexity\" of a 2D image.\n",
    "\n",
    "\n",
    "Cateogory Distribution: 357 benign, 212 malignant"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1-2: Characteristics of the dataset\n",
    "\n",
    "Answer some questions about the structure of our data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean_data\n",
    "\n",
    "original_cancer_dataset = clean_data.generate_clean_dataframe()\n",
    "cancer_dataset = original_cancer_dataset.reset_index()\n",
    "cancer_dataset.drop(\"ID\", axis=1, inplace=True)\n",
    "cancer_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3-5: Simple Linear Regression\n",
    "\n",
    "We will start with the scatter plots that we used to discuss correlation during Week 2, now through the perspective of using a Simple Linear Regression to characterize these relationships. We are going to introduce the concept of the __Coefficient of Determination__: $R^2$. \n",
    "\n",
    "$$ R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}$$\n",
    "\n",
    "Where \n",
    "- SSE = error of sum of squares\n",
    "- SSR = regression sum of squares\n",
    "- SST = total sum of squares\n",
    "\n",
    "This value conceptually represents the \"amount of variance in y is explained by x\" and can be thought of as how well the model fits the training data. This is a seperate concept to the Pearson Correlation Coefficient, $\\rho$, which in the cases of a __strictly linear regression__, $R^2 = \\rho^2$. \n",
    "\n",
    "You can also calculate the adjusted $R^2$ which accounts for the model size and parameters. This way, you can compare the adjusted $R^2$ of simple models to the adjusted $R^2$ of complex models, which you can't necessarily do if you just use  $R^2$. \n",
    "\n",
    "$$R^2_{adj} = 1 - \\frac{\\frac{SSE}{n-k-1}}{\\frac{SST}{n - 1}}$$\n",
    "\n",
    "Where \n",
    "- n = number of data points\n",
    "- k number of independent variables\n",
    "\n",
    "Generally speaking, $R^2$ is used to characterize the fit of a model, where as $\\rho$ is used to characterize the relationship between two variables. \n",
    "\n",
    "We will perform a simple linear regression between the response variable, y, and the feature, x. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order for us to use diagnosis as a variable, we need to turn it from categorical (M, B) into numerical values. We will set \"M\" to 1, and \"B\" to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset['diagnosis'].replace([\"M\",\"B\"], [1,0], inplace=True)\n",
    "cancer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots of the various features\n",
    "def calculate_correlation(x,y):\n",
    "    \"\"\"calculate pearson correlation\"\"\"\n",
    "    return calculate_mean((x - calculate_mean(x)).transpose() * (y - calculate_mean(y))) / np.sqrt(calculate_variance(x) * calculate_variance(y))\n",
    "\n",
    "def calculate_mean(x):\n",
    "    \"\"\"get the sample mean\"\"\"\n",
    "    return np.sum(x) / len(x)\n",
    "\n",
    "def calculate_variance(x):\n",
    "    \"\"\"calculate variance of the dataset\"\"\"\n",
    "    return calculate_mean((x - calculate_mean(x))**2)\n",
    "\n",
    "def simple_linear_regression(x,y, ax):\n",
    "    regression = linear_model.LinearRegression() \n",
    "    regression.fit(x.values.reshape(-1,1),y)\n",
    "    coef_of_determination = regression.score(x.values.reshape(-1,1),y)\n",
    "    x_range = np.linspace(min(x), max(x))\n",
    "    plt.plot(x_range, regression.coef_*x_range + regression.intercept_, \"--\",)\n",
    "    plt.text(1.01, 0.98, r\"$\\beta_1 = {0:.2f}$\".format(regression.coef_[0]),\n",
    "             ha='left', va='top', size =LABEL_FONT,\n",
    "             transform=ax.transAxes)\n",
    "    plt.text(1.01, 0.95, r\"$\\beta_0 = {0:.2f}$\".format(regression.intercept_),\n",
    "         ha='left', va='top', size =LABEL_FONT,\n",
    "         transform=ax.transAxes)\n",
    "    plt.text(1.01, 0.92, r\"$R^2 = {0:.2f}$\".format(coef_of_determination),\n",
    "         ha='left', va='top', size =LABEL_FONT,\n",
    "         transform=ax.transAxes)\n",
    "    \n",
    "@widgets.interact(x=list(cancer_dataset), y=list(cancer_dataset))    \n",
    "def make_scatterplot(x=\"mean_radius\",y=\"mean_area\"):\n",
    "    \"\"\"make scatterplot with correlation value and regplot\"\"\"\n",
    "    colors=[\"#e28743\", \"#1e81b0\"]\n",
    "    corr = calculate_correlation(cancer_dataset[x], cancer_dataset[y])\n",
    "    index = int(corr > 0.5)\n",
    "    plt.title(r\"correlation: $\\rho = ${:.3f}\".format(corr), color= \"grey\", size=TITLE_FONT)\n",
    "    df = cancer_dataset.reset_index()\n",
    "    ax = sns.scatterplot(data=df, x=x, y=y, alpha=0.4, hue=\"diagnosis\")\n",
    "    simple_linear_regression(cancer_dataset[x], cancer_dataset[y], ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Statistical tests and beta coefficients\n",
    "\n",
    "We did not spend a lot of time discussing statistical tests with respect to associated p-values, but the interpretation of these values are the same as we've discussed throughout the quarter. You can use hypothesis testing to interrogate your regression coefficients. Here the most common test is using the null hypothesis of $\\mathcal{H}_0: \\beta_i = 0$, which corresponds to the _t_-statistic. If you do this test on __binary__ or __boolean__ variable, you get the same exact result as a t-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_ind_ttest(feature=\"mean_symmetry\", dataset = original_cancer_dataset, welch=False):\n",
    "    \"\"\"\n",
    "    run two sample t-tests\n",
    "    For the motivated, visit \n",
    "    https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html \n",
    "    to see the documentation to see how to write the code for a t-test.\"\"\"\n",
    "    cat1 = dataset.xs(\"M\", level=1)\n",
    "    cat2 = dataset.xs(\"B\", level=1)\n",
    "    df = dataset.reset_index()\n",
    "    f, (ax_hist, ax_box) = plt.subplots(2, sharex=True)\n",
    "\n",
    "    sns.boxplot(data=df, x=feature, y=df[\"diagnosis\"], ax=ax_box)\n",
    "    sns.histplot(data=df, x=feature, hue=df[\"diagnosis\"], ax=ax_hist, stat=\"probability\", kde=True)\n",
    "    if welch: tstat, pvalue = ttest(cat1[feature], cat2[feature], equal_var=False)\n",
    "    else: tstat, pvalue = ttest(cat1[feature], cat2[feature])\n",
    "    print(f\"p-value: {pvalue:.2e}\")\n",
    "    plt.show()\n",
    "    make_scatterplot(\"diagnosis\", feature)\n",
    "    slope, intercept, r_value, reg_p_value, std_err = stats.linregress(cancer_dataset[\"diagnosis\"], cancer_dataset[feature])\n",
    "\n",
    "    print(f\"p-value: {reg_p_value:.2e}\")    \n",
    "    \n",
    "run_ind_ttest()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ind_ttest(feature=\"mean_fractal_dimension\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions 6-8: Multiple Linear Regression \n",
    "\n",
    "We have other variables that we can use to explore the relationships in this dataset. We can use multiple linear regression to include more variables, where we are drawing a multi-dimensional shape, rather than simply a line, as our model. The plot below is currently the same as the one we looked at above with mean_area and mean_radius. Try adding some other variables into your regression by checking the boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def linear_regression(df, feature_cols, response_col, standardized = False):\n",
    "    \"\"\"\n",
    "    Use linear_model to run a linear regression using sklearn\n",
    "    \n",
    "    \"\"\"\n",
    "    X = df[feature_cols]\n",
    "    y = df[response_col]\n",
    "    if standardized:\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        y = StandardScaler().fit_transform(y.values.reshape(-1, 1))\n",
    "    regression = linear_model.LinearRegression() \n",
    "    regression.fit(X,y)\n",
    "    \n",
    "    try:\n",
    "        print('Intercept of MLR model is {0:0.2f}'.format(regression.intercept_))\n",
    "    except TypeError:\n",
    "        print('Intercept of MLR model is {0:0.2f}'.format(regression.intercept_[0]))\n",
    "    print('Regression Coefficients: ')\n",
    "    for feature, coef in zip(feature_cols, regression.coef_.flatten()):\n",
    "        print(f'{feature} ~ {coef:.2f}')\n",
    "    return regression.predict(X), regression.score(X,y)\n",
    "\n",
    "def error_distribution(true, pred, hue=None):\n",
    "    ax = plt.subplots(figsize=(12, 8))\n",
    "    error = true-pred\n",
    "    if hue is not None: \n",
    "        sns.kdeplot(error, hue=hue, shade=True, alpha=.2)\n",
    "    else:\n",
    "        sns.kdeplot(error, shade=True, alpha=.2)\n",
    "    sns.despine()\n",
    "\n",
    "    plt.xlabel(r'Residuals ($\\epsilon$)')\n",
    "    plt.title('Error', size=TITLE_FONT)\n",
    "    plt.show()\n",
    "    \n",
    "def parity_plot(true, pred, r_squared=None, title='', hue=None):\n",
    "    \"\"\"\n",
    "    plot true vs the predicted data\n",
    "    inputs: 2 list-like (arrays) data structures\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize=(10, 8))\n",
    "    if hue is not None:\n",
    "        sns.scatterplot(x=true, y=pred, hue=hue)\n",
    "    else: \n",
    "        sns.scatterplot(x=true, y=pred)\n",
    "    min_value = min(min(true), min(pred))\n",
    "    max_value = max(max(true), max(pred))\n",
    "    plt.plot([min_value, max_value],[min_value, max_value], '--', label=\"parity\")\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    ax.set_box_aspect(1)\n",
    "    sns.despine()\n",
    "    plt.text(1.01, 0.98, r\"$R^2 = {0:.2f}$\".format(r_squared),\n",
    "         ha='left', va='top', size =LABEL_FONT,\n",
    "         transform=ax.transAxes)\n",
    "    plt.title('Parity Plot: {}'.format(title), size=TITLE_FONT)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    # error_distribution(true, pred, hue)\n",
    "\n",
    "\n",
    "def run_regression(feature_cols = \n",
    "                   ['mean_radius', \n",
    "                    'mean_texture', \n",
    "                    'mean_perimeter', \n",
    "                    'mean_smoothness', \n",
    "                    'mean_compactness', \n",
    "                    'mean_concavity', \n",
    "                    'mean_concave_points', \n",
    "                    'mean_symmetry', \n",
    "                    'mean_fractal_dimension'], \n",
    "                     response_col='mean_area',\n",
    "                     standardized=False,\n",
    "                     parity=True):\n",
    "    y_pred, r_squared = linear_regression(cancer_dataset, feature_cols, response_col, standardized = standardized)\n",
    "    if parity: parity_plot(cancer_dataset[response_col], y_pred.flatten(), r_squared, hue=cancer_dataset[\"diagnosis\"])\n",
    "\n",
    "\n",
    "@widgets.interact(response=list(cancer_dataset))   \n",
    "def regression_wrapper(response=\"mean_radius\",\n",
    "                       diagnosis=False,\n",
    "                       radius=False, \n",
    "                       texture=False, \n",
    "                       perimeter=False, \n",
    "                       area=True,\n",
    "                       smoothness=False, \n",
    "                       compactness=False, \n",
    "                       concavity=False, \n",
    "                       concave_points=False, \n",
    "                       symmetry=False, \n",
    "                       fractal_dimension=False):\n",
    "    features = []\n",
    "    if diagnosis: features.append(\"diagnosis\")\n",
    "    if radius: features.append(\"mean_radius\")\n",
    "    if texture: features.append(\"mean_texture\")\n",
    "    if perimeter: features.append(\"mean_perimeter\")\n",
    "    if area: features.append(\"mean_area\")\n",
    "    if smoothness: features.append(\"mean_smoothness\")\n",
    "    if compactness: features.append(\"mean_compactness\")    \n",
    "    if concavity: features.append(\"mean_concavity\")\n",
    "    if concave_points: features.append(\"mean_concave_points\")\n",
    "    if symmetry: features.append(\"mean_symmetry\")\n",
    "    if fractal_dimension: features.append(\"mean_fractal_dimension\")\n",
    "        \n",
    "\n",
    "    run_regression(feature_cols=features, response_col = response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9: Normalization\n",
    "\n",
    "We need to be careful about how we interpret parameters in relation to each other. One method that I can use to compare coefficients is to __standardize__ the features. This will effectively shift the distribution of all of our features to have a Standard Normal ($\\mu=0,\\sigma^2=1$) distribution. We can then compare the coefficients to each other, and interpret the magnitude within the model. \n",
    "\n",
    "Note: This transformation prevents you from making a direct interpretation of the coefficient as it relates to the actual value, and is now unitless. You should interpret these as being \"model units\". This transformation also means that the y-intercept will be zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@widgets.interact(response=list(cancer_dataset))   \n",
    "def regression_wrapper(response=\"mean_area\", \n",
    "                       radius=True, \n",
    "                       texture=True, \n",
    "                       perimeter=True, \n",
    "                       area=False,\n",
    "                       smoothness=True, \n",
    "                       compactness=True, \n",
    "                       concavity=True, \n",
    "                       concave_points=True, \n",
    "                       symmetry=True, \n",
    "                       fractal_dimension=True):\n",
    "    features = []\n",
    "    if radius: features.append(\"mean_radius\")\n",
    "    if texture: features.append(\"mean_texture\")\n",
    "    if perimeter: features.append(\"mean_perimeter\")\n",
    "    if area: features.append(\"mean_area\")\n",
    "    if smoothness: features.append(\"mean_smoothness\")\n",
    "    if compactness: features.append(\"mean_compactness\")    \n",
    "    if concavity: features.append(\"mean_concavity\")\n",
    "    if concave_points: features.append(\"mean_concave_points\")\n",
    "    if symmetry: features.append(\"mean_symmetry\")\n",
    "    if fractal_dimension: features.append(\"mean_fractal_dimension\")\n",
    "        \n",
    "\n",
    "    run_regression(feature_cols=features, response_col = response, standardized=True, parity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can ignore the code below and skip to the relevant example for question 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic(x):\n",
    "    # Default - 2x^2 + 2\n",
    "    return 2*x**2 + 2\n",
    "\n",
    "def generate_noisy_data(function, noise_std, n=10, measurement_std=.2, initial_value=0, x_max=3, plot=True):\n",
    "    \"\"\"\n",
    "    This function generates noisy data with a certain amount of error applied to the function response.\n",
    "    The error is normally distributed around the noise_std.\n",
    "    \"\"\"\n",
    "    x = np.linspace(0, x_max, n) \n",
    "    x_noise = np.random.normal(0, measurement_std, len(x))\n",
    "    x += x_noise\n",
    "    y_noise = np.random.normal(0, noise_std, len(x))\n",
    "    y = function(x) + initial_value\n",
    "    y += y_noise\n",
    "    if plot:\n",
    "        plt.plot(x, y, 'C0.', label='data')\n",
    "        x_func = np.linspace(0, max(x)+measurement_std)\n",
    "        y_func = function(x_func) + initial_value\n",
    "        plt.plot(x_func, y_func, 'C0--', label='function')\n",
    "        plt.fill_between(x_func, y_func+noise_std, y_func-noise_std,\n",
    "                         alpha=0.1)          # Transparency of the fill\n",
    "        plt.title(r'$ y = 2x^2 + 2$ with noise (std of {})'.format(noise_std))\n",
    "        plt.legend(loc='best')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.xlim(0, max(x)+measurement_std)\n",
    "        plt.show()\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_model(x, y, x_model, y_model, title = '',r_squared=None, x_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Plotter function.\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.plot(x,y, 'o', label='data')\n",
    "    if x_test is not None: plt.plot(x_test,y_test,\"o\", label='test')\n",
    "    plt.plot(x_model, y_model, '--', label='model')\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.xlim(0, max(x))\n",
    "    plt.title(title)\n",
    "    plt.text(1.01, 0.98, r\"$R^2 = {0:.2f}$\".format(r_squared),\n",
    "         ha='left', va='top', size =LABEL_FONT,\n",
    "         transform=plt.gca().transAxes)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def polynomial_feature_example(x, y, degrees=6, x_test=None, y_test=None):\n",
    "    \"\"\"\n",
    "    Perform regularization on a polynomial feature set. \n",
    "    \"\"\"\n",
    "    poly_transform = PolynomialFeatures(degree=degrees, include_bias = False)\n",
    "    x_poly = poly_transform.fit_transform(x.reshape(-1,1))\n",
    "    \n",
    "    #Regularization techniques need to be scaled in order to work properly\n",
    "    x_scaler = StandardScaler().fit(x_poly)\n",
    "    y_scaler = StandardScaler().fit(y.reshape(-1,1))\n",
    "    x_poly_z = x_scaler.transform(x_poly)\n",
    "    y_z = y_scaler.transform(y.reshape(-1,1))\n",
    "    \n",
    "    #Code to perform the model fitting and parameter estimation\n",
    "    #Least Squares problem\n",
    "    plt.suptitle('Linear Regression', fontsize=20, fontweight='bold')\n",
    "    lm_poly = linear_model.LinearRegression(fit_intercept=True)\n",
    "    lm_poly.fit(x_poly_z,y_z)\n",
    "\n",
    "    x_model = np.linspace(min(x), max(x), 150).reshape(-1,1)\n",
    "    x_model_transform = poly_transform.fit_transform(x_model)\n",
    "    x_model_transform_z = x_scaler.transform(x_model_transform)\n",
    "    \n",
    "    \n",
    "    y_model = lm_poly.predict(x_model_transform_z)*y_scaler.scale_ + y_scaler.mean_\n",
    "    \n",
    "    #********************************************************************************\n",
    "    # Coefficients from scaled model can be transformed back into original units\n",
    "    # This code is outside the scope of this class and can be ignored. \n",
    "    \n",
    "    unscaled_coefficients = (lm_poly.coef_ * y_scaler.scale_ / x_scaler.scale_).flatten()\n",
    "    \n",
    "    poly_terms = [r'$({0:.3f})x^{{{1}}}$'.format(coef, i+1) for i, coef in enumerate(unscaled_coefficients)\n",
    "                 if coef != 0]\n",
    "    \n",
    "    unscaled_intercept = lm_poly.intercept_*y_scaler.scale_ + y_scaler.mean_ \\\n",
    "                            - sum(unscaled_coefficients*x_scaler.mean_)\n",
    "        \n",
    "    intercept_str = r'${0:.1f} + $'.format(unscaled_intercept[0])\n",
    "    title =  intercept_str + r'$+$'.join(poly_terms)\n",
    "    #********************************************************************************\n",
    "    r_squared = lm_poly.score(x_poly_z,y_z)\n",
    "    ax = plot_model(x, y, x_model, y_model, title=title, r_squared = r_squared, x_test=x_test, y_test=y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions 10: Underdefined and Overdefined\n",
    "\n",
    "In class we discussed two different types of problems where the number of features does not equal the number of observations. We are going to generate data from a quadratic equation with some noise, and then we are going to try and fit a new linear model:\n",
    "\n",
    "$$ y = \\beta_0 + \\sum_{i=1}^{degrees} \\beta_i x^i$$ \n",
    "\n",
    "Where we are going to take an original dataset of 1 feature, and generate multiple new features from the original feature. From this equation, if we set `degrees` equal to 1, we are doing a simple linear regression. \n",
    "\n",
    "If we set `degrees` equal to 2, we are fitting a MLR model with $x$ and $x^2$ as our features. \n",
    "Creating polynomial features is one way that we can do __feature engineering__, a common activity in machine learning.\n",
    "Another benefit of using polynomial features is so that we can represent all of the data and the model in a 2 dimensional plot. \n",
    "\n",
    "\n",
    "Answer the questions in the classword regarding the two different types of problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@widgets.interact_manual(n=(0,100), degrees=(1,12))\n",
    "def run_polynomial_experiment(n=20, degrees=1):\n",
    "    x_data, y_data = generate_noisy_data(quadratic, 2, n, plot=False)\n",
    "    polynomial_feature_example(x_data, y_data, degrees=degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: A little bit more about feature engineering\n",
    "When it comes to Machine Learning, you don't always have the features that you want to make the best model. This concept is closely tied to the ideas of Neural Networks (a core part of Machine Learning, which is outside the scope of this class) where the goal is to __extract__ the appropriate features for a model. However, if we want to have control over what types of features we put into our model (in order for us to know explicitly how the model works) we can use feature engineering. \n",
    "\n",
    "We already covered one aspect of feature engineering in the classwork, specifically polynomial features. There are two other techniques that would have been appropriate for this dataset. Let's tackle the following problem: The model was not good at the extremes of the dataset, and it seems like we could draw two lines that were better than one line. \n",
    "\n",
    "__One-Hot Encoding__: We can change the `diagnosis` feature into two features: `malignant` and `benign` and assign a value of 1 or 0 to each feature. These features would be __mutually exclusive__, meaning that both features cannot have a value of 1.\n",
    "\n",
    "__Interactions__: We could multiply every feature by every other feature (or selection of features) to add interactions in the model. For example:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2} & x_{1,3}\\\\\n",
    "x_{2,1} & x_{1,2} & x_{2,p}\\\\\n",
    "\\vdots & \\dots & \\vdots\\\\ \n",
    "x_{n,1} & x_{n,2} & x_{n,}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_{interactions} = \n",
    "\\begin{bmatrix}\n",
    "x_{1,1}*x_{1,2} & x_{1,1}*x_{1,3} & x_{1,2}*x_{1,3}\\\\\n",
    "x_{2,1}*x_{2,2} & x_{2,1}*x_{2,3} & x_{2,2}*x_{2,3}\\\\\n",
    "\\vdots & \\dots & \\vdots\\\\ \n",
    "x_{n,1}*x_{n,2} & x_{n,1}*x_{n,3} & x_{n,2}*x_{n,3}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Try to consider how combining these two feature engineering techniques would effectively make two different models, one when `malignant` is equal to 1, and one when `benign` is equal to 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: MLR by hand\n",
    "\n",
    "For those motivated to understand MLR a little deeper, some code to run an MLR by hand is shown below.\n",
    "\n",
    "There is an extra step in here that we didn't focus on in class that's required to fit the intercept.\n",
    "Let's start with a feature matrix, $X$, and a response vector, $Y$:\n",
    "\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots\\\\ \n",
    "y_n\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & \\dots & x_{1,p}\\\\\n",
    "x_{2,1} & \\dots & x_{2,p}\\\\\n",
    "\\vdots & \\dots & \\vdots\\\\ \n",
    "x_{n,1} & \\dots & x_{n,p}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We will pad the $X$ with a column of $1$'s before we solve the linear regression problem: \n",
    "\n",
    "$$\n",
    "\\tilde{X} = \n",
    "\\begin{bmatrix}\n",
    "1&x_{1,1} & \\dots & x_{1,p}\\\\\n",
    "1&x_{2,1} & \\dots & x_{2,p}\\\\\n",
    "1&\\vdots & \\dots & \\vdots\\\\ \n",
    "1&x_{n,1} & \\dots & x_{n,p}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If you want to fit an intercept:\n",
    "$$\n",
    "B = (\\tilde{X}^T\\tilde{X})^{-1}(\\tilde{X}^TY)\n",
    "$$\n",
    "\n",
    "Where B: \n",
    "$$\n",
    "B = \n",
    "\\begin{bmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots\\\\ \n",
    "\\beta_n\\\\\n",
    "\\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "Therefore, when you solve for $\\beta_0$, the feature that it's using is a column of 1, which means that it is a constant. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
